
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>B-Eye-O-Marker CNN-V2 (BiNLOP-3) — Accessible Inference UI</title>
<meta name="description" content="On-device eye condition analyzer (Camera/Upload/Manual), ONNX Runtime Web, fully accessible with TTS." />
<style>
  :root{
    --bg:#0b1020;--panel:#121a2f;--panel-2:#0e1530;--text:#eef2ff;--muted:#b7c1e6;
    --accent:#68d391;--accent-2:#60a5fa;--warn:#fbbf24;--err:#f87171;
    --card:#0d1530;--hl:#1c2750;--focus:#93c5fd;--ok:#34d399;
  }
  *{box-sizing:border-box}
  html,body{height:100%}
  body{
    margin:0;background:linear-gradient(180deg,#0a0f22 0%, #101733 100%);
    color:var(--text);font:16px/1.5 system-ui,Segoe UI,Roboto,Helvetica,Arial;
  }
  a{color:var(--accent-2)}
  .wrap{max-width:1200px;margin:0 auto;padding:24px}
  header{display:flex;flex-wrap:wrap;gap:16px;align-items:center;justify-content:space-between;margin-bottom:16px}
  .brand{display:flex;align-items:center;gap:12px}
  .brand h1{font-size:20px;margin:0}
  .badge{background:var(--hl);border:1px solid #2a386b;color:#cfe1ff;padding:4px 8px;border-radius:999px;font-size:12px}
  .panel{background:var(--panel);border:1px solid #1e2954;border-radius:16px;padding:16px}
  .grid{display:grid;gap:16px}
  @media(min-width:900px){.grid-2{grid-template-columns:1.2fr .8fr}}
  .controls{display:flex;flex-wrap:wrap;gap:12px;align-items:center}
  button, input[type="file"], select{
    background:var(--hl);color:var(--text);border:1px solid #2a386b;border-radius:10px;padding:10px 14px;
    font-weight:600;cursor:pointer
  }
  button:disabled{opacity:.6;cursor:not-allowed}
  button.primary{background:linear-gradient(90deg,#2563eb,#06b6d4);border:none}
  button.warn{background:linear-gradient(90deg,#f59e0b,#ef4444);border:none}
  .tabs{display:flex;gap:8px;flex-wrap:wrap}
  .tab{padding:8px 12px;border-radius:999px;border:1px solid #2a386b;background:#0e1530;color:#cfe1ff;cursor:pointer}
  .tab[aria-selected="true"]{background:var(--accent-2);color:#051225;border-color:transparent}
  .section{display:none}
  .section.active{display:block}
  .kpi{display:grid;grid-template-columns:repeat(4,minmax(0,1fr));gap:10px}
  .kpi div{background:var(--panel-2);border:1px solid #243159;border-radius:12px;padding:12px}
  .kpi h3{margin:0 0 6px 0;font-size:12px;color:var(--muted)}
  .kpi p{margin:0;font-size:18px;font-weight:700}
  .flex{display:flex;gap:12px;flex-wrap:wrap;align-items:center}
  .video-wrap{position:relative;background:#000;border-radius:12px;overflow:hidden;border:1px solid #1f2a52}
  video, canvas{max-width:100%;display:block}
  .sr-only{position:absolute;left:-10000px;width:1px;height:1px;overflow:hidden}
  .live{border-left:4px solid var(--accent);padding-left:12px}
  .chart-card{background:var(--panel-2);border:1px solid #243159;border-radius:12px;padding:12px}
  .rec-card{background:var(--panel-2);border:1px solid #243159;border-radius:12px;padding:12px}
  .footer{opacity:.85;font-size:12px;margin-top:16px}
  .pill{display:inline-block;padding:2px 8px;border-radius:999px;background:#0d1a3a;border:1px solid #2a386b;color:#d2dfff;font-size:12px}
  .progress-outer{background:#0b132b;border-radius:999px;overflow:hidden;border:1px solid #243159}
  .progress-inner{height:10px;background:linear-gradient(90deg,#22c55e,#06b6d4)}
  .skip a{position:absolute;left:-10000px;top:auto;width:1px;height:1px;overflow:hidden}
  .skip a:focus{position:static;width:auto;height:auto;background:#fff;color:#000;padding:4px 8px}
  .tts-controls{display:flex;gap:10px;flex-wrap:wrap;align-items:center}
  .label{font-size:12px;color:var(--muted)}
</style>
</head>
<body>
<div class="skip"><a href="#main">Skip to main content</a></div>
<div class="wrap" role="application" aria-label="B-Eye-O-Marker accessible inference interface">
  <header aria-label="Header and model controls">
    <div class="brand" aria-label="Branding">
      <span class="badge" aria-label="Model family">B-Eye-O-Marker</span>
      <h1 id="appTitle">CNN-V2 (BiNLOP-3) — Local ONNX</h1>
    </div>
    <div class="controls" role="group" aria-label="Model loading and runtime controls">
      <button id="btnLoadModel" class="primary" aria-describedby="modelStatus">Load ONNX Model</button>
      <select id="backend" aria-label="Execution provider">
        <option value="webgpu">WebGPU (fastest, preferred)</option>
        <option value="webgl">WebGL</option>
        <option value="wasm">WASM (fallback)</option>
      </select>
      <span id="modelStatus" class="pill" role="status" aria-live="polite">Model: not loaded</span>
    </div>
  </header>

  <main id="main" class="grid grid-2" aria-label="Main content">
    <section class="panel" aria-label="Modes and inputs">
      <h2>Choose Mode</h2>
      <div class="tabs" role="tablist" aria-label="Input modes">
        <button class="tab" role="tab" id="tabCamera" aria-controls="secCamera" aria-selected="true">Camera</button>
        <button class="tab" role="tab" id="tabUpload" aria-controls="secUpload" aria-selected="false">Upload Photo</button>
        <button class="tab" role="tab" id="tabManual" aria-controls="secManual" aria-selected="false">Manual</button>
      </div>

      <!-- Camera -->
      <div id="secCamera" class="section active" role="tabpanel" aria-labelledby="tabCamera">
        <div class="video-wrap" aria-label="Camera preview">
          <video id="video" autoplay playsinline muted aria-label="Live camera feed"></video>
        </div>
        <div class="flex" style="margin-top:10px">
          <button id="btnStartCam">Start Camera</button>
          <button id="btnSnap" class="primary" disabled>Take Picture & Analyze</button>
          <button id="btnStopCam" class="warn" disabled>Stop Camera</button>
        </div>
        <canvas id="canvas" class="sr-only" aria-hidden="true"></canvas>
      </div>

      <!-- Upload -->
      <div id="secUpload" class="section" role="tabpanel" aria-labelledby="tabUpload">
        <input id="fileImage" type="file" accept="image/*" aria-label="Upload an eye photo for analysis" />
        <div id="uploadPreview" class="video-wrap" style="margin-top:10px" aria-label="Uploaded image preview"></div>
        <div class="flex" style="margin-top:10px">
          <button id="btnAnalyzeUpload" class="primary" disabled>Analyze Uploaded Photo</button>
        </div>
      </div>

      <!-- Manual -->
      <div id="secManual" class="section" role="tabpanel" aria-labelledby="tabManual">
        <label class="label" for="manualSelect">Select your condition manually (CNN bypassed):</label><br/>
        <select id="manualSelect" aria-label="Manual condition selector">
          <option value="Normal">Normal</option>
          <option value="DiabeticRetinopathy">Diabetic Retinopathy (DR)</option>
          <option value="DiabeticMacularEdema">Diabetic Macular Edema (DME)</option>
          <option value="AgeRelatedMacularDegeneration">Age-Related Macular Degeneration (AMD)</option>
          <option value="Glaucoma">Glaucoma</option>
          <option value="RetinalDetachment">Retinal Detachment (RD)</option>
          <option value="EpiretinalMembrane">Epiretinal Membrane (ERM)</option>
          <option value="VitreousDetachment">Vitreous Detachment</option>
          <option value="Other">Other / Unspecified</option>
        </select>
        <div class="flex" style="margin-top:10px">
          <button id="btnManual" class="primary">Generate Guidance</button>
        </div>
      </div>

      <div class="panel" style="margin-top:16px" aria-label="Text-to-speech controls">
        <h2>Text-to-Speech</h2>
        <div class="tts-controls">
          <label for="voiceSel">Voice:</label>
          <select id="voiceSel" aria-label="TTS Voice"></select>
          <label for="rateSel">Rate:</label>
          <input id="rateSel" type="range" min="0.6" max="1.4" step="0.05" value="1" aria-label="TTS rate" />
          <label for="volSel">Volume:</label>
          <input id="volSel" type="range" min="0.2" max="1" step="0.05" value="1" aria-label="TTS volume" />
          <label><input id="autoSpeak" type="checkbox" aria-label="Automatically speak results" /> Auto-speak results</label>
          <button id="btnSpeak" aria-label="Speak the current results">Speak Now</button>
          <button id="btnStopSpeak" aria-label="Stop speaking">Stop</button>
        </div>
      </div>
    </section>

    <section class="panel" aria-label="Results and insights">
      <h2>Results</h2>

      <div class="kpi" aria-label="Key performance indicators">
        <div><h3>Inference Latency</h3><p id="latency">—</p></div>
        <div><h3>Backend</h3><p id="kBackend">—</p></div>
        <div><h3>Image Size</h3><p id="kImg">—</p></div>
        <div><h3>Top Class</h3><p id="kTop">—</p></div>
      </div>

      <div class="chart-card" style="margin-top:12px" aria-label="Probability distribution chart">
        <h3>Probability Distribution</h3>
        <canvas id="chart" role="img" aria-label="Bar chart of class probabilities"></canvas>
        <div class="sr-only" id="chartText" aria-live="polite"></div>
      </div>

      <div class="rec-card" style="margin-top:12px" aria-label="Recommendations and context">
        <h3>Recommendations & Overview</h3>
        <div id="recommendations" class="live" aria-live="polite">No results yet.</div>
      </div>

      <div class="rec-card" style="margin-top:12px" aria-label="Additional metrics">
        <h3>Additional Metrics</h3>
        <ul id="metricsList">
          <li>Preprocessing time: <span id="prepTime">—</span></li>
          <li>Model input shape: <span id="inputShape">—</span></li>
          <li>Output logits shape: <span id="outShape">—</span></li>
        </ul>
        <div class="progress-outer" aria-label="Confidence of top class">
          <div id="confBar" class="progress-inner" style="width:0%"></div>
        </div>
      </div>

      <div class="footer" aria-label="Important note">
        <strong>Disclaimer:</strong> This tool is for informational and educational purposes only and is <em>not</em> a medical device. It does not provide medical advice, diagnosis, or treatment. Always consult a qualified clinician for concerns about your eyes or vision.
      </div>
    </section>
  </main>
</div>

<!-- Libraries -->
<script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>

<script>
/* ==========================
   Accessible, TTS-first, Local ONNX UI
   ========================== */

const CLASSES = [
  "Normal",
  "DiabeticRetinopathy",
  "DiabeticMacularEdema",
  "AgeRelatedMacularDegeneration",
  "Glaucoma",
  "RetinalDetachment",
  "EpiretinalMembrane",
  "VitreousDetachment",
  "Other"
];
const IMG_SIZE = 384; // training default (HxW)
const CHANNELS = 3;   // RGB
const MODEL_DB_NAME = 'b_eye_o_marker_model_db';
const MODEL_DB_KEY  = 'onnx_bytes';

let ortSession = null;
let chart = null;
let currentResultText = '';
let voices = [];
let lastImageRGBA = null;

// ======================= IndexedDB: persist model for auto-load =======================
function openIDB() {
  return new Promise((resolve, reject) => {
    const req = indexedDB.open(MODEL_DB_NAME, 1);
    req.onupgradeneeded = (e) => {
      const db = e.target.result;
      if (!db.objectStoreNames.contains('files')) db.createObjectStore('files');
    };
    req.onsuccess = () => resolve(req.result);
    req.onerror = () => reject(req.error);
  });
}
async function idbPutModel(bytes) {
  const db = await openIDB();
  return new Promise((resolve, reject) => {
    const tx = db.transaction('files','readwrite');
    tx.objectStore('files').put(bytes, MODEL_DB_KEY);
    tx.oncomplete = () => resolve();
    tx.onerror = () => reject(tx.error);
  });
}
async function idbGetModel() {
  const db = await openIDB();
  return new Promise((resolve, reject) => {
    const tx = db.transaction('files','readonly');
    const req = tx.objectStore('files').get(MODEL_DB_KEY);
    req.onsuccess = () => resolve(req.result || null);
    req.onerror = () => reject(req.error);
  });
}

// ======================= TTS =======================
const tts = {
  synth: window.speechSynthesis,
  utter: null,
  speak(text) {
    if (!text) return;
    this.stop();
    const u = new SpeechSynthesisUtterance(text);
    u.rate = parseFloat(document.getElementById('rateSel').value || '1');
    u.volume = parseFloat(document.getElementById('volSel').value || '1');
    const vSel = document.getElementById('voiceSel');
    const voiceName = vSel?.value;
    if (voices.length && voiceName) {
      const v = voices.find(v => v.name === voiceName);
      if (v) u.voice = v;
    }
    this.utter = u;
    this.synth.speak(u);
  },
  stop(){ if (this.synth.speaking) this.synth.cancel(); }
};
function populateVoices() {
  voices = tts.synth.getVoices().filter(v => v.lang.startsWith('en') || v.localService);
  const sel = document.getElementById('voiceSel');
  sel.innerHTML = '';
  voices.forEach(v => {
    const opt = document.createElement('option');
    opt.value = v.name;
    opt.textContent = `${v.name} — ${v.lang}${v.default ? ' (default)' : ''}`;
    sel.appendChild(opt);
  });
}
if ('speechSynthesis' in window) {
  speechSynthesis.onvoiceschanged = populateVoices;
  setTimeout(populateVoices, 300);
}

// ======================= UI helpers =======================
function setStatus(text, good=false){
  const el = document.getElementById('modelStatus');
  el.textContent = text;
  el.style.background = good ? 'linear-gradient(90deg,#22c55e,#06b6d4)' : '';
}
function setKPI({latency, backend, img, top}){
  document.getElementById('latency').textContent = latency ? `${latency.toFixed(1)} ms` : '—';
  document.getElementById('kBackend').textContent = backend || '—';
  document.getElementById('kImg').textContent = img || '—';
  document.getElementById('kTop').textContent = top || '—';
}
function sigmoid(x){ return 1/(1+Math.exp(-x)); }

// ======================= Chart =======================
function renderChart(probs){
  const ctx = document.getElementById('chart').getContext('2d');
  const data = {
    labels: CLASSES,
    datasets: [{
      label: 'Probability',
      data: probs.map(p => Math.round(p*1000)/10),
      borderWidth: 1
    }]
  };
  if (chart) chart.destroy();
  chart = new Chart(ctx, {
    type: 'bar',
    data,
    options: {
      responsive: true,
      scales: {
        y: { beginAtZero: true, max: 100, title: {display:true, text:'%'} }
      },
      plugins: {
        legend: { display: false },
        tooltip: { enabled: true }
      },
      animation: { duration: 300 }
    }
  });
  // Text alternative for TTS + screen readers
  const text = probs.map((p,i)=>`${CLASSES[i]}: ${(p*100).toFixed(1)}%`).join('; ');
  const chartText = document.getElementById('chartText');
  chartText.textContent = `Probabilities — ${text}`;
  return text;
}

// ======================= Recommendations =======================
const RECS = {
  Normal: [
    "Maintain regular eye exams as advised by your optometrist/ophthalmologist.",
    "Manage screen time with 20-20-20 rule (every 20 minutes, look 20 feet away for 20 seconds).",
    "Keep systemic risk factors healthy: glucose, blood pressure, lipids."
  ],
  DiabeticRetinopathy: [
    "Strict glycemic control and blood pressure management are critical.",
    "Schedule a comprehensive dilated retinal exam promptly.",
    "Ask about retinal imaging (OCT, fundus) and potential anti-VEGF therapy if indicated."
  ],
  DiabeticMacularEdema: [
    "Macular changes can threaten central vision—seek an ophthalmology visit soon.",
    "Ask about OCT imaging to quantify fluid and response to therapy.",
    "Discuss anti-VEGF or steroid therapy if confirmed."
  ],
  AgeRelatedMacularDegeneration: [
    "If metamorphopsia or distortion: urgent evaluation for neovascular AMD.",
    "Consider Amsler grid self-monitoring and lifestyle optimization (smoking cessation).",
    "Discuss AREDS2 supplementation with your clinician for intermediate AMD."
  ],
  Glaucoma: [
    "Glaucoma is often asymptomatic—arrange IOP, visual field, and OCT RNFL testing.",
    "Adherence to drops (if prescribed) is crucial; ask about side-effect alternatives.",
    "Family members may benefit from screening."
  ],
  RetinalDetachment: [
    "Emergency: sudden floaters/flash/curtain warrants urgent eye hospital assessment.",
    "Avoid strenuous activity until assessed.",
    "Surgery (pneumatic retinopexy, scleral buckle, vitrectomy) may be required."
  ],
  EpiretinalMembrane: [
    "Monitor metamorphopsia and acuity; OCT confirms diagnosis.",
    "If significant distortion, discuss surgical peeling (pars plana vitrectomy).",
    "Optimize lighting and contrast for reading tasks."
  ],
  VitreousDetachment: [
    "Common with age; if new floaters/flashes or shadow, get urgent evaluation to exclude tears.",
    "Avoid eye rubbing; monitor symptoms for change.",
    "Follow up as advised; retinal tears can occur around PVD onset."
  ],
  Other: [
    "Condition unspecified; seek comprehensive clinical evaluation.",
    "Document symptoms onset, severity, and any systemic factors.",
    "Bring prior imaging or prescriptions to your visit."
  ]
};
function buildOverview(topClass, probs){
  const idx = CLASSES.indexOf(topClass);
  const pTop = probs[idx] || 0;
  const recs = RECS[topClass] || RECS.Other;
  const rank = [...probs].map((p,i)=>[i,p]).sort((a,b)=>b[1]-a[1]).map(x=>CLASSES[x[0]]);
  const txt = [
    `Top condition: ${topClass} (${(pTop*100).toFixed(1)}%)`,
    `Runner-ups: ${rank.slice(1,4).join(', ')}`,
    `Recommendations: ${recs.join(' ')}`
  ].join('. ');
  return {recs, txt};
}
function updateRecommendations(topClass, probs){
  const {recs, txt} = buildOverview(topClass, probs);
  const el = document.getElementById('recommendations');
  el.innerHTML = `
    <p><strong>Top condition:</strong> ${topClass}</p>
    <ul>${recs.map(r=>`<li>${r}</li>`).join('')}</ul>
    <p style="opacity:.85">Always confirm with a qualified clinician; this is not medical advice.</p>
  `;
  return txt;
}

// ======================= Image handling =======================
// Replace existing imageToTensor with this function
async function imageToTensor(imgEl){
  const t0 = performance.now();
  const canvas = document.createElement('canvas');
  canvas.width = IMG_SIZE; canvas.height = IMG_SIZE;
  const ctx = canvas.getContext('2d');
  // Direct resize (no letterbox) — matches torchvision.Resize((IMG_SIZE, IMG_SIZE))
  ctx.drawImage(imgEl, 0, 0, canvas.width, canvas.height);

  const imgData = ctx.getImageData(0,0,IMG_SIZE,IMG_SIZE);
  lastImageRGBA = imgData;
  const N = IMG_SIZE*IMG_SIZE;
  const dataR = new Float32Array(N);
  const dataG = new Float32Array(N);
  const dataB = new Float32Array(N);
  const d = imgData.data;
  for (let i=0,p=0; i<d.length; i+=4, p++){
    dataR[p] = d[i] / 255.0;
    dataG[p] = d[i+1] / 255.0;
    dataB[p] = d[i+2] / 255.0;
  }
  const input = new ort.Tensor('float32', new Float32Array(CHANNELS*N), [1, CHANNELS, IMG_SIZE, IMG_SIZE]);
  input.data.set(dataR, 0);
  input.data.set(dataG, N);
  input.data.set(dataB, 2*N);
  const prepMs = performance.now() - t0;
  return {input, prepMs, size:`${IMG_SIZE}x${IMG_SIZE}`};
}

// ======================= ONNX Runtime =======================
async function initSession(modelBytes=null){
  const provider = document.getElementById('backend').value;
  document.getElementById('kBackend').textContent = provider.toUpperCase();
  const opts = {
    executionProviders: [provider],
    graphOptimizationLevel: 'all',
    enableMemPattern: true,
    enableCpuMemArena: true
  };
  if (provider === 'wasm'){
    // WASM options
    opts.wasm = { numThreads: Math.max(2, navigator.hardwareConcurrency ? Math.floor(navigator.hardwareConcurrency/2) : 4) };
  }
  if (modelBytes){
    ortSession = await ort.InferenceSession.create(modelBytes, opts);
  } else {
    const cached = await idbGetModel();
    if (!cached) return null;
    ortSession = await ort.InferenceSession.create(cached, opts);
  }
  return ortSession;
}

async function ensureModelLoaded(autoSpeakNotice=true){
  if (ortSession) return true;
  const sess = await initSession(null);
  if (sess){
    setStatus('Model: loaded from cache ✓', true);
    if (document.getElementById('autoSpeak').checked && autoSpeakNotice){
      tts.speak('Model loaded from cache. You can start.');
    }
    return true;
  }
  setStatus('Model: not loaded — click “Load ONNX Model”');
  return false;
}

// ======================= Inference =======================
async function runInferenceFromElement(imgEl){
  if (!await ensureModelLoaded()) return;
  const {input, prepMs, size} = await imageToTensor(imgEl);
  const t0 = performance.now();
  const feeds = { input };
  const out = await ortSession.run(feeds);
  const logits = out.logits?.data || Object.values(out)[0].data; // safeguard
  const latency = performance.now() - t0;

  // Multi-label: apply sigmoid per class
  const probs = Array.from({length: CLASSES.length}, (_,i) => sigmoid(logits[i] || 0));
  const topIdx = probs
    .map((p,i)=>[i,p])
    .sort((a,b)=>b[1]-a[1])[0][0];
  const topClass = CLASSES[topIdx];
  const topProb = probs[topIdx];

  // UI updates
  setKPI({latency, backend: document.getElementById('backend').value.toUpperCase(), img:size, top:`${topClass} (${(topProb*100).toFixed(1)}%)`});
  const chartAlt = renderChart(probs);
  document.getElementById('inputShape').textContent = `[1, ${CHANNELS}, ${IMG_SIZE}, ${IMG_SIZE}]`;
  document.getElementById('outShape').textContent = `[1, ${CLASSES.length}] (logits)`;
  document.getElementById('prepTime').textContent = `${prepMs.toFixed(1)} ms`;
  document.getElementById('confBar').style.width = `${Math.round(topProb*100)}%`;

  const recText = updateRecommendations(topClass, probs);
  currentResultText = `Inference complete. ${chartAlt}. ${recText}`;
  if (document.getElementById('autoSpeak').checked) {
    tts.speak(currentResultText);
  }
}

// ======================= Camera =======================
let stream = null;
const video = document.getElementById('video');
async function startCamera(){
  try{
    stream = await navigator.mediaDevices.getUserMedia({video:{facingMode:"user", width:{ideal:1280}, height:{ideal:720}}, audio:false});
    video.srcObject = stream;
    document.getElementById('btnSnap').disabled = false;
    document.getElementById('btnStopCam').disabled = false;
  }catch(e){
    alert('Could not access camera: ' + e.message);
  }
}
function stopCamera(){
  if (stream){ stream.getTracks().forEach(t=>t.stop()); stream=null; }
  document.getElementById('btnSnap').disabled = true;
  document.getElementById('btnStopCam').disabled = true;
}
async function snapshotAndAnalyze(){
  if (!video.videoWidth){ alert('Camera not ready'); return; }
  await runInferenceFromElement(video);
}

// ======================= Upload =======================
const uploadPreview = document.getElementById('uploadPreview');
let uploadedImgEl = null;
document.getElementById('fileImage').addEventListener('change', (e)=>{
  const f = e.target.files?.[0];
  if (!f) return;
  const url = URL.createObjectURL(f);
  uploadPreview.innerHTML = '';
  const img = document.createElement('img');
  img.src = url; img.alt = 'Uploaded eye image preview';
  img.onload = ()=> URL.revokeObjectURL(url);
  uploadPreview.appendChild(img);
  uploadedImgEl = img;
  document.getElementById('btnAnalyzeUpload').disabled = false;
});

// ======================= Manual mode =======================
function runManual(){
  const val = document.getElementById('manualSelect').value;
  const probs = CLASSES.map(c => c===val ? 0.92 : 0.01); // present a distribution
  const chartAlt = renderChart(probs);
  setKPI({latency: NaN, backend: 'Manual', img: '—', top: `${val} (manual)`});
  document.getElementById('inputShape').textContent = '— (manual mode)';
  document.getElementById('outShape').textContent = `[1, ${CLASSES.length}] (synthetic)`;
  document.getElementById('prepTime').textContent = '—';
  document.getElementById('confBar').style.width = `92%`;
  const recText = updateRecommendations(val, probs);
  currentResultText = `Manual selection. ${chartAlt}. ${recText}`;
  if (document.getElementById('autoSpeak').checked) tts.speak(currentResultText);
}

// ======================= Tabs =======================
function activateTab(id){
  const tabs = document.querySelectorAll('.tab');
  const sections = document.querySelectorAll('.section');
  tabs.forEach(t => t.setAttribute('aria-selected', t.id === id));
  sections.forEach(s => s.classList.toggle('active', s.id === document.getElementById(id).getAttribute('aria-controls')));
}

// ======================= Events =======================
document.getElementById('btnStartCam').addEventListener('click', startCamera);
document.getElementById('btnStopCam').addEventListener('click', stopCamera);
document.getElementById('btnSnap').addEventListener('click', snapshotAndAnalyze);
document.getElementById('btnAnalyzeUpload').addEventListener('click', async ()=>{
  if (!uploadedImgEl){ alert('Upload an image first.'); return; }
  await runInferenceFromElement(uploadedImgEl);
});
document.getElementById('btnManual').addEventListener('click', runManual);
document.getElementById('btnSpeak').addEventListener('click', ()=> tts.speak(currentResultText || 'There are no results yet.'));
document.getElementById('btnStopSpeak').addEventListener('click', ()=> tts.stop());
document.getElementById('backend').addEventListener('change', async ()=>{
  // re-init session on backend change (using cached model)
  if (ortSession) { ortSession = null; }
  const ok = await ensureModelLoaded(false);
  if (ok) setStatus('Model: ready ✓', true);
});
document.getElementById('tabCamera').addEventListener('click', ()=>activateTab('tabCamera'));
document.getElementById('tabUpload').addEventListener('click', ()=>activateTab('tabUpload'));
document.getElementById('tabManual').addEventListener('click', ()=>activateTab('tabManual'));

// Load model from online URL
const MODEL_URL = 'https://huggingface.co/Dawntasy/B-Eye-O-Marker_CNN-V2.1/resolve/main/B-Eye-O-Marker_CNN-V2_Swish.onnx';

async function loadModelFromURL(){
  setStatus('Downloading model…');
  try{
    console.log('Fetching model from:', MODEL_URL);
    const response = await fetch(MODEL_URL, { mode: 'cors' });
    console.log('Response status:', response.status, response.statusText);
    if (!response.ok) throw new Error(`HTTP ${response.status}: ${response.statusText}`);
    
    const contentLength = response.headers.get('content-length');
    console.log('Model size:', contentLength ? `${Math.round(contentLength/1024/1024)}MB` : 'unknown');
    
    const arr = await response.arrayBuffer();
    console.log('Downloaded bytes:', arr.byteLength);
    
    setStatus('Loading model…');
    await idbPutModel(arr);
    
    console.log('Creating ONNX session...');
    ortSession = await ort.InferenceSession.create(arr, {
      executionProviders: [document.getElementById('backend').value],
      graphOptimizationLevel: 'all'
    });
    
    console.log('Model loaded successfully');
    setStatus('Model: loaded ✓', true);
    document.getElementById('kBackend').textContent = document.getElementById('backend').value.toUpperCase();
    if (document.getElementById('autoSpeak').checked) tts.speak('Model loaded successfully. You can start.');
  }catch(err){
    console.error('Model loading error:', err);
    setStatus('Model load failed', false);
    alert('Model load failed: ' + err.message + '\nCheck console for details.');
  }
}

document.getElementById('btnLoadModel').addEventListener('click', loadModelFromURL);

// Auto-load model on page load
window.addEventListener('load', async ()=>{
  const ok = await ensureModelLoaded(); // attempts cache first
  if (!ok) {
    await loadModelFromURL(); // download if not cached
  } else {
    setStatus('Model: loaded from cache ✓', true);
  }
});

// ======================= Accessibility niceties =======================
document.getElementById('appTitle').setAttribute('tabindex','0');
document.getElementById('recommendations').setAttribute('tabindex','0');
</script>
</body>
</html>